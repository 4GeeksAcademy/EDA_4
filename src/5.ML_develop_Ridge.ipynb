{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "import pickle\n",
    "from pickle import dump\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('/workspaces/EDA_4/data/processed/heart_prevalence_y_train.csv')\n",
    "y_test = pd.read_csv('/workspaces/EDA_4/data/processed/heart_prevalence_y_test.csv')\n",
    "X_train_norm = pd.read_csv('/workspaces/EDA_4/data/interim/heart_prevalence_X_train_std.csv')\n",
    "X_test_norm = pd.read_csv('/workspaces/EDA_4/data/interim/heart_prevalence_X_test_std.csv')\n",
    "X_train = pd.read_csv('/workspaces/EDA_4/data/processed/heart_prevalence_X_train.csv')\n",
    "X_test = pd.read_csv('/workspaces/EDA_4/data/processed/heart_prevalence_X_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore specific data conversion warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I embark on the model training phase, I've chosen to include both normalized and non-normalized datasets this time. The purpose is to scrutinize how data normalization impacts the model's performance and make comparisons between the outcomes under different preprocessing conditions.\n",
    "\n",
    "Following the methodology established in the previous script, I'll use a systematic loop to identify the optimal number of variables for our model. This step is crucial, as it aims to strike a balance between the model's complexity and its ability to generalize, ultimately contributing to a more effective and robust predictive tool.\n",
    "\n",
    "Once we determine the optimal configuration, the next step involves evaluating the model's performance metrics, with a specific focus on the mean squared error and the coefficient of determination. This evaluation aims to highlight the subtle differences between the training and testing datasets, serving as a crucial indicator of the model's ability to generalize well to unseen data.\n",
    "\n",
    "By incorporating both normalized and non-normalized datasets, my goal is to gain nuanced insights into how preprocessing decisions influence the model's performance. This comprehensive exploration aligns with best practices in model development and will play a pivotal role in guiding decisions for model selection and fine-tuning in the subsequent phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.27978595682405133\n",
      "Coefficient of Determination (R-squared): 0.9574473435853107\n",
      "Best model is selected with 0.5 of features\n",
      "-----------------------------------\n",
      "Intercept (a): [8.37149865]\n",
      "Coefficients (b): [[-0.16040501  0.68951099 -0.24149325  0.54356876 -0.57176282  0.54451463]]\n",
      "-----------------------------------\n",
      "Root Mean Squared Error for Training set: 0.27978595682405133\n",
      "Coefficient of Determination for Training set: 0.9338943689562693\n",
      "-----------------------------------\n",
      "Root Mean Squared Error for Testing set: 0.5216644287205902\n",
      "Coefficient of Determination for Testing set: 0.8706226686953741\n",
      "-----------------------------------\n",
      "The difference between Train and Test in Root Mean Squared Error is -0.24187847189653883.\n",
      "The difference between Train and Test in Coefficient of Determination is 0.06327170026089524.\n"
     ]
    }
   ],
   "source": [
    "# Lists to store root mean squared errors (rmss) and R-squared values (r2s)\n",
    "rmss = []\n",
    "r2s = []\n",
    "\n",
    "# Percentage values for feature selection\n",
    "percents = [1, 0.8, 0.7, 0.6, 0.5]\n",
    "\n",
    "# Iterate over different percentages for feature selection\n",
    "for p in percents:\n",
    "    \n",
    "    # SelectKBest with f_regression for feature selection\n",
    "    selection_model = SelectKBest(f_regression, k=int(len(X_train_norm.columns) * p))\n",
    "    selection_model.fit(X_train_norm, y_train)\n",
    "    ix = selection_model.get_support()\n",
    "\n",
    "    # Transform datasets based on selected features\n",
    "    X_train_sel = pd.DataFrame(selection_model.transform(X_train_norm), columns=X_train_norm.columns.values[ix])\n",
    "    X_test_sel = pd.DataFrame(selection_model.transform(X_test_norm), columns=X_test_norm.columns.values[ix])\n",
    "\n",
    "    # Save the selection model for potential future use\n",
    "    dump(selection_model, open(f\"/workspaces/EDA_4/models/selection_model_ridge{p}.pk\", \"wb\"))\n",
    "\n",
    "    # Ridge Regression model\n",
    "    ridge_model = Ridge(alpha=0.1, max_iter=300)\n",
    "    ridge_model.fit(X_train_sel, y_train)\n",
    "\n",
    "    # Predictions on the training set\n",
    "    y_pred = ridge_model.predict(X_train_sel)\n",
    "    \n",
    "    # Calculate and store root mean squared error (rmse) and R-squared value (r2)\n",
    "    rmss.append(math.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "    r2s.append(r2_score(y_train, y_pred))\n",
    "\n",
    "# Find the index of the best model based on maximum rmse and r2 values\n",
    "best_rmss = rmss.index(max(rmss))\n",
    "best_r2s = r2s.index(max(r2s))\n",
    "\n",
    "# Print the evaluation metrics for the best model\n",
    "print(f\"Root Mean Squared Error: {rmss[best_rmss]}\")\n",
    "print(f\"Coefficient of Determination (R-squared): {r2s[best_r2s]}\")\n",
    "print(f\"Best model is selected with {percents[best_rmss]} of features\")\n",
    "\n",
    "# Load the selection model of the best-performing percentage\n",
    "selection_model = pickle.load(open(f\"/workspaces/EDA_4/models/selection_model_ridge0.5.pk\", \"rb\"))\n",
    "ix = selection_model.get_support()\n",
    "\n",
    "# Transform datasets based on selected features\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train_norm), columns=X_train_norm.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test_norm), columns=X_test_norm.columns.values[ix])\n",
    "\n",
    "# Create a Ridge Regression model\n",
    "model = Ridge(alpha=0.1, max_iter=300)\n",
    "model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Display model coefficients and intercept\n",
    "print('-' * 35)\n",
    "print(f\"Intercept (a): {model.intercept_}\")\n",
    "print(f\"Coefficients (b): {model.coef_}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_pred = model.predict(X_train_sel)\n",
    "e1 = math.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r1 = r2_score(y_train, y_pred)\n",
    "print(f\"Root Mean Squared Error for Training set: {e1}\")\n",
    "print(f\"Coefficient of Determination for Training set: {r1}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = model.predict(X_test_sel)\n",
    "e2 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error for Testing set: {e2}\")\n",
    "print(f\"Coefficient of Determination for Testing set: {r2}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Print the difference between training and testing performance metrics\n",
    "print(f\"The difference between Train and Test in Root Mean Squared Error is {e1 - e2}.\")\n",
    "print(f\"The difference between Train and Test in Coefficient of Determination is {r1 - r2}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.2825484423493222\n",
      "Coefficient of Determination (R-squared): 0.9574454972798395\n",
      "Best model is selected with 0.5 of features\n",
      "-----------------------------------\n",
      "Intercept (a): [2.63161161]\n",
      "Coefficients (b): [[-2.84166300e-05  1.63809491e-01 -9.72199961e-02  2.61134043e-01\n",
      "  -2.67989164e-01  1.25618512e+00]]\n",
      "-----------------------------------\n",
      "Root Mean Squared Error for Training set: 0.2825484423493222\n",
      "Coefficient of Determination for Training set: 0.9325825276763562\n",
      "-----------------------------------\n",
      "Root Mean Squared Error for Testing set: 0.5105536241453681\n",
      "Coefficient of Determination for Testing set: 0.876075131352427\n",
      "-----------------------------------\n",
      "The difference between Train and Test in Root Mean Squared Error is -0.2280051817960459.\n",
      "The difference between Train and Test in Coefficient of Determination is 0.056507396323929204.\n"
     ]
    }
   ],
   "source": [
    "# Lists to store root mean squared errors (rmss) and R-squared values (r2s)\n",
    "rmss = []\n",
    "r2s = []\n",
    "\n",
    "# Percentage values for feature selection\n",
    "percents = [1, 0.8, 0.7, 0.6, 0.5]\n",
    "\n",
    "# Iterate over different percentages for feature selection\n",
    "for p in percents:\n",
    "    \n",
    "    # SelectKBest with f_regression for feature selection\n",
    "    selection_model = SelectKBest(f_regression, k=int(len(X_train.columns) * p))\n",
    "    selection_model.fit(X_train, y_train)\n",
    "    ix = selection_model.get_support()\n",
    "\n",
    "    # Transform datasets based on selected features\n",
    "    X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns=X_train.columns.values[ix])\n",
    "    X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns=X_test.columns.values[ix])\n",
    "\n",
    "    # Save the selection model for potential future use\n",
    "    dump(selection_model, open(f\"/workspaces/EDA_4/models/selection_model_ridge{p}.pk\", \"wb\"))\n",
    "\n",
    "    # Ridge Regression model\n",
    "    ridge_model = Ridge(alpha=0.1, max_iter=300)\n",
    "    ridge_model.fit(X_train_sel, y_train)\n",
    "\n",
    "    # Predictions on the training set\n",
    "    y_pred = ridge_model.predict(X_train_sel)\n",
    "    \n",
    "    # Calculate and store root mean squared error (rmse) and R-squared value (r2)\n",
    "    rmss.append(math.sqrt(mean_squared_error(y_train, y_pred)))\n",
    "    r2s.append(r2_score(y_train, y_pred))\n",
    "\n",
    "# Find the index of the best model based on maximum rmse and r2 values\n",
    "best_rmss = rmss.index(max(rmss))\n",
    "best_r2s = r2s.index(max(r2s))\n",
    "\n",
    "# Print the evaluation metrics for the best model\n",
    "print(f\"Root Mean Squared Error: {rmss[best_rmss]}\")\n",
    "print(f\"Coefficient of Determination (R-squared): {r2s[best_r2s]}\")\n",
    "print(f\"Best model is selected with {percents[best_rmss]} of features\")\n",
    "\n",
    "# Load the selection model of the best-performing percentage\n",
    "selection_model = pickle.load(open(f\"/workspaces/EDA_4/models/selection_model_ridge0.5.pk\", \"rb\"))\n",
    "ix = selection_model.get_support()\n",
    "\n",
    "# Transform datasets based on selected features\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns=X_train.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns=X_test.columns.values[ix])\n",
    "\n",
    "# Create a Ridge Regression model\n",
    "model = Ridge(alpha=0.1, max_iter=300)\n",
    "model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Display model coefficients and intercept\n",
    "print('-' * 35)\n",
    "print(f\"Intercept (a): {model.intercept_}\")\n",
    "print(f\"Coefficients (b): {model.coef_}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_pred = model.predict(X_train_sel)\n",
    "e1 = math.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r1 = r2_score(y_train, y_pred)\n",
    "print(f\"Root Mean Squared Error for Training set: {e1}\")\n",
    "print(f\"Coefficient of Determination for Training set: {r1}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = model.predict(X_test_sel)\n",
    "e2 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error for Testing set: {e2}\")\n",
    "print(f\"Coefficient of Determination for Testing set: {r2}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Print the difference between training and testing performance metrics\n",
    "print(f\"The difference between Train and Test in Root Mean Squared Error is {e1 - e2}.\")\n",
    "print(f\"The difference between Train and Test in Coefficient of Determination is {r1 - r2}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting both training sessions, it becomes evident that the model trained with non-normalized data exhibits a smaller difference between the training and testing datasets. Consequently, it displays less overfitting and higher overall quality. I opt for this model, save its configuration, and proceed to fine-tune its hyperparameters for optimization in the subsequent steps.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the selection model of the best-performing percentage\n",
    "selection_model = pickle.load(open(f\"/workspaces/EDA_4/models/selection_model_ridge0.5.pk\", \"rb\"))\n",
    "ix = selection_model.get_support()\n",
    "\n",
    "# Transform datasets based on selected features\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns=X_train.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns=X_test.columns.values[ix])\n",
    "\n",
    "# Create a Ridge Regression model\n",
    "model = Ridge(alpha=0.1, max_iter=300)\n",
    "best_model = model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Save the best model\n",
    "dump(best_model, open(f\"/workspaces/EDA_4/models/best_model_ridge.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we approach the final phase of the project, our objective is to fine-tune the model's hyperparameters, aiming to enhance its overall effectiveness while mitigating overfitting. To accomplish this, we'll load the previously selected model, and through a systematic exploration of hyperparameters using GridSearch, we intend to identify the optimal combination that maximizes performance.\n",
    "\n",
    "Utilizing a dictionary, we'll specify the hyperparameters to be optimized, and the GridSearch algorithm from sklearn will systematically search through the provided parameter grid to find the configuration that yields the best results. It's worth noting that, given the relatively small size of our dataset, the computational overhead of GridSearch is manageable, and the process is expected to conclude efficiently.\n",
    "\n",
    "This meticulous hyperparameter tuning step is crucial for refining the model's predictive capabilities and ensuring that it generalizes well to unseen data. The goal is to strike a balance that maximizes performance without compromising the model's ability to adapt to new information. The outcome of this optimization process will mark the culmination of our efforts, providing us with a well-tailored and effective predictive model for the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the selection model of the best-performing percentage\n",
    "selection_model = pickle.load(open(f\"/workspaces/EDA_4/models/selection_model_ridge0.5.pk\", \"rb\"))\n",
    "ix = selection_model.get_support()\n",
    "\n",
    "# Transform datasets based on selected features\n",
    "X_train_sel = pd.DataFrame(selection_model.transform(X_train), columns=X_train.columns.values[ix])\n",
    "X_test_sel = pd.DataFrame(selection_model.transform(X_test), columns=X_test.columns.values[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=Ridge(random_state=24),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1.0, 10.0],\n",
       "                         &#x27;copy_X&#x27;: [True, False],\n",
       "                         &#x27;fit_intercept&#x27;: [True, False],\n",
       "                         &#x27;max_iter&#x27;: [None, 100, 500, 1000],\n",
       "                         &#x27;random_state&#x27;: [None, 42, 100],\n",
       "                         &#x27;solver&#x27;: [&#x27;auto&#x27;, &#x27;svd&#x27;, &#x27;cholesky&#x27;, &#x27;lsqr&#x27;],\n",
       "                         &#x27;tol&#x27;: [0.0001, 0.001, 0.01]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=Ridge(random_state=24),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1.0, 10.0],\n",
       "                         &#x27;copy_X&#x27;: [True, False],\n",
       "                         &#x27;fit_intercept&#x27;: [True, False],\n",
       "                         &#x27;max_iter&#x27;: [None, 100, 500, 1000],\n",
       "                         &#x27;random_state&#x27;: [None, 42, 100],\n",
       "                         &#x27;solver&#x27;: [&#x27;auto&#x27;, &#x27;svd&#x27;, &#x27;cholesky&#x27;, &#x27;lsqr&#x27;],\n",
       "                         &#x27;tol&#x27;: [0.0001, 0.001, 0.01]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(random_state=24)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(random_state=24)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=Ridge(random_state=24),\n",
       "             param_grid={'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
       "                         'copy_X': [True, False],\n",
       "                         'fit_intercept': [True, False],\n",
       "                         'max_iter': [None, 100, 500, 1000],\n",
       "                         'random_state': [None, 42, 100],\n",
       "                         'solver': ['auto', 'svd', 'cholesky', 'lsqr'],\n",
       "                         'tol': [0.0001, 0.001, 0.01]})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary for Ridge Model hyperparameters\n",
    "hyper = {\n",
    "    'alpha': [1e-3, 1e-2, 1e-1, 1.0, 10.0],            # Regularization parameter\n",
    "    'fit_intercept': [True, False],                     # Whether to fit the intercept                        \n",
    "    'copy_X': [True, False],                            # Whether to make a copy of X\n",
    "    'max_iter': [None, 100, 500, 1000],                 # Maximum number of iterations\n",
    "    'tol': [1e-4, 1e-3, 1e-2],                          # Tolerance for convergence\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr'],     # Solver method\n",
    "    'random_state': [None, 42, 100],                    # Seed for random number generation\n",
    "}\n",
    "\n",
    "# Store the model into a variable\n",
    "ridge_regression = Ridge(random_state=24)\n",
    "\n",
    "# Hyperparameter tuning using Grid Search\n",
    "grid = GridSearchCV(ridge_regression, hyper, cv = 5)\n",
    "grid.fit(X_train_sel, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Intercept (a): [5.88958773]\n",
      "Coefficients (b): [[-4.56273848e-05  1.88729360e-01 -1.32780122e-01  1.97686126e-01\n",
      "  -1.53890686e-01  3.30592230e-01]]\n",
      "-----------------------------------\n",
      "Root Mean Squared Error for Training set: 0.3016723228171599\n",
      "Coefficient of Determination for Training set: 0.9231475768642361\n",
      "-----------------------------------\n",
      "Root Mean Squared Error for Testing set: 0.5053895283499601\n",
      "Coefficient of Determination for Testing set: 0.8785693782344194\n",
      "-----------------------------------\n",
      "The difference between Train and Test in Root Mean Squared Error is -0.2037172055328002.\n",
      "The difference between Train and Test in Coefficient of Determination is 0.04457819862981671.\n"
     ]
    }
   ],
   "source": [
    "# Get the best hyperparameters from the grid search results\n",
    "best_hyper = grid.best_params_\n",
    "\n",
    "# Create an optimized Ridge Regression model using the best hyperparameters\n",
    "opt_model = Ridge(**best_hyper)\n",
    "opt_model.fit(X_train_sel, y_train)\n",
    "\n",
    "# Display model coefficients and intercept\n",
    "print('-' * 35)\n",
    "print(f\"Intercept (a): {opt_model.intercept_}\")\n",
    "print(f\"Coefficients (b): {opt_model.coef_}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_pred = opt_model.predict(X_train_sel)\n",
    "e1 = math.sqrt(mean_squared_error(y_train, y_pred))\n",
    "r1 = r2_score(y_train, y_pred)\n",
    "print(f\"Root Mean Squared Error for Training set: {e1}\")\n",
    "print(f\"Coefficient of Determination for Training set: {r1}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "y_pred = opt_model.predict(X_test_sel)\n",
    "e2 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Root Mean Squared Error for Testing set: {e2}\")\n",
    "print(f\"Coefficient of Determination for Testing set: {r2}\")\n",
    "print('-' * 35)\n",
    "\n",
    "# Print the difference between training and testing performance metrics\n",
    "print(f\"The difference between Train and Test in Root Mean Squared Error is {e1 - e2}.\")\n",
    "print(f\"The difference between Train and Test in Coefficient of Determination is {r1 - r2}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the absolute best and optimized model :)\n",
    "dump(opt_model, open(f\"/workspaces/EDA_4/models/opt_model_ridge.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion (final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a series of deliberations and strategic decisions, we have successfully further reduced the overfitting to an impressive 0.045. Considering the practical goals of the project, we find this outcome satisfactory. Importantly, each decision and step taken throughout the process has been geared towards progressively minimizing the gap between the training and testing datasets. This continuous refinement has contributed significantly to the enhancement of our model.\n",
    "\n",
    "In summary, we explored two models, linear regression and Ridge, with the latter providing a modest yet notable improvement in addressing the overfitting issue. Subsequently, by introducing non-normalized data into this Ridge model, we achieved even better results. As a result, we have chosen to proceed with the non-normalized Ridge model for the final optimization phase.\n",
    "\n",
    "In this optimization phase, we fine-tuned the hyperparameters, leading to a further enhancement of results. The overall progress from the initial model training in the previous script to the latest phase reflects a significant improvement. We are content with the achieved outcome and, for the time being, consider the project concluded."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
